{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Portfolio3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TinaSorensen/CALDISS-SDS-phd-19/blob/master/Portfolio3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7PFt6x9Ow0c",
        "colab_type": "text"
      },
      "source": [
        "Take the following text and transform it into a list of lists with with each element being a tokenized sentence. Remove stopwords, lower all tokens and keep only alpha-numeric tokens.\n",
        "\n",
        "\"I’ve been called many things in my life, but never an optimist. That was fine by me. I believed pessimists lived in a constant state of pleasant surprise: if you always expected the worst, things generally turned out better than you imagined. The only real problem with pessimism, I figured, was that too much of it could accidentally turn you into an optimist.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2urDiUMNf9g",
        "colab_type": "code",
        "outputId": "37a14300-08ef-4ade-d0a5-efa36a3e4b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import nltk #this part is needed on colab.\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "#----------------------------------------\n",
        "\n",
        "# Tokenizing sentences\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Tokenizing words\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMWKAIosN43O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text=\"I’ve been called many things in my life, but never an optimist. That was fine by me. I believed pessimists lived in a constant state of pleasant surprise: if you always expected the worst, things generally turned out better than you imagined. The only real problem with pessimism, I figured, was that too much of it could accidentally turn you into an optimist.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6dUitERN_H8",
        "colab_type": "code",
        "outputId": "d840d7b7-5337-41d1-9e1c-21c2b63db6d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Let's get our stences.\n",
        "# Note that the full-stops at the end of each sentence are still there\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I’ve been called many things in my life, but never an optimist.', 'That was fine by me.', 'I believed pessimists lived in a constant state of pleasant surprise: if you always expected the worst, things generally turned out better than you imagined.', 'The only real problem with pessimism, I figured, was that too much of it could accidentally turn you into an optimist.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uvhn3WRwOUZo",
        "colab_type": "code",
        "outputId": "4d69e59c-63de-4bf1-eaea-205b24cda778",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Use word_tokenize to tokenize the third sentence: tokenized_sent\n",
        "tokenized_sent = word_tokenize(sentences[1])\n",
        "\n",
        "# Make a set of unique tokens in the entire scene: unique_tokens\n",
        "unique_tokens = set(word_tokenize(text))\n",
        "\n",
        "print(tokenized_sent)\n",
        "print(unique_tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['That', 'was', 'fine', 'by', 'me', '.']\n",
            "{'been', 'than', 'out', 'better', 'imagined', ',', 'fine', 'too', 'figured', 'many', 'I', 'me', 'in', 'pleasant', 'was', 'always', 'an', 'expected', 'you', 'generally', 'only', 'could', ':', 'state', 'called', 've', 'believed', '’', 'but', 'lived', 'The', 'of', 'it', 'life', 'into', 'That', 'the', 'much', 'constant', 'pessimism', 'never', 'surprise', 'if', 'with', 'that', 'a', 'accidentally', 'problem', 'things', 'my', 'real', 'optimist', '.', 'by', 'worst', 'pessimists', 'turned', 'turn'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqT9lfNkObH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english')) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzRhAZlAOhAw",
        "colab_type": "code",
        "outputId": "872ddd77-6b80-4d27-bb12-862adedb4c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        }
      },
      "source": [
        "[word.lower() for word in word_tokenize(text) if word not in stop_words and word.isalnum()]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'called',\n",
              " 'many',\n",
              " 'things',\n",
              " 'life',\n",
              " 'never',\n",
              " 'optimist',\n",
              " 'that',\n",
              " 'fine',\n",
              " 'i',\n",
              " 'believed',\n",
              " 'pessimists',\n",
              " 'lived',\n",
              " 'constant',\n",
              " 'state',\n",
              " 'pleasant',\n",
              " 'surprise',\n",
              " 'always',\n",
              " 'expected',\n",
              " 'worst',\n",
              " 'things',\n",
              " 'generally',\n",
              " 'turned',\n",
              " 'better',\n",
              " 'imagined',\n",
              " 'the',\n",
              " 'real',\n",
              " 'problem',\n",
              " 'pessimism',\n",
              " 'i',\n",
              " 'figured',\n",
              " 'much',\n",
              " 'could',\n",
              " 'accidentally',\n",
              " 'turn',\n",
              " 'optimist']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM7KpUDQPa_s",
        "colab_type": "text"
      },
      "source": [
        "The link below holds a datasewt with ~10k #OKBoomer tweets from the days 10-21 Nov.\n",
        "\n",
        "https://github.com/CALDISS-AAU/sdsphd19_coursematerials/raw/master/data/tweets_boomer.zip\n",
        "\n",
        "Use elements from the above code to make a list of the most common hashtags (you have to get the hashtags from the text, not using the column containing them already)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft6GZPZTPcp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', -1) #to see more text\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKnqHkuWPiZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizing Tweets made easy!\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = TweetTokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDcveHxpPlnS",
        "colab_type": "code",
        "outputId": "db06db32-cd7f-49c5-d539-4c06a17153f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "# download and open some #OKBoomer tweets from trump_tweet_data_archive\n",
        "\n",
        "boomer_tweets_df = pd.read_json('https://github.com/CALDISS-AAU/sdsphd19_coursematerials/raw/master/data/tweets_boomer.zip')\n",
        "boomer_tweets_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>date</th>\n",
              "      <th>timezone</th>\n",
              "      <th>place</th>\n",
              "      <th>tweet</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>cashtags</th>\n",
              "      <th>user_id</th>\n",
              "      <th>user_id_str</th>\n",
              "      <th>username</th>\n",
              "      <th>name</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>link</th>\n",
              "      <th>retweet</th>\n",
              "      <th>nlikes</th>\n",
              "      <th>nreplies</th>\n",
              "      <th>nretweets</th>\n",
              "      <th>quote_url</th>\n",
              "      <th>search</th>\n",
              "      <th>near</th>\n",
              "      <th>geo</th>\n",
              "      <th>source</th>\n",
              "      <th>user_rt_id</th>\n",
              "      <th>user_rt</th>\n",
              "      <th>retweet_id</th>\n",
              "      <th>reply_to</th>\n",
              "      <th>retweet_date</th>\n",
              "      <th>lang</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1197612747715837952</td>\n",
              "      <td>1197612747715837952</td>\n",
              "      <td>2019-11-21 20:28:24</td>\n",
              "      <td>2019-11-21 20:28:24</td>\n",
              "      <td>UTC</td>\n",
              "      <td></td>\n",
              "      <td>Suddenly, #OkBoomer is trending again.  https://twitter.com/Fox35Matt/status/1197185353330909184 …</td>\n",
              "      <td>[#okboomer]</td>\n",
              "      <td>[]</td>\n",
              "      <td>262050686</td>\n",
              "      <td>262050686</td>\n",
              "      <td>DanClarkSports</td>\n",
              "      <td>Dan</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>https://twitter.com/DanClarkSports/status/1197612747715837953</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>https://twitter.com/Fox35Matt/status/1197185353330909184</td>\n",
              "      <td>#okboomer</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[{'user_id': '262050686', 'username': 'DanClarkSports'}]</td>\n",
              "      <td></td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1197612550403297280</td>\n",
              "      <td>1197612550403297280</td>\n",
              "      <td>2019-11-21 20:27:37</td>\n",
              "      <td>2019-11-21 20:27:37</td>\n",
              "      <td>UTC</td>\n",
              "      <td></td>\n",
              "      <td>I like my role in this 🤷‍♂️ #OkBoomer #GenX pic.twitter.com/6RI4bAsaml</td>\n",
              "      <td>[#okboomer, #genx]</td>\n",
              "      <td>[]</td>\n",
              "      <td>41444665</td>\n",
              "      <td>41444665</td>\n",
              "      <td>sinths</td>\n",
              "      <td>Sven Thomas</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>https://twitter.com/sinths/status/1197612550403297280</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>#okboomer</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[{'user_id': '41444665', 'username': 'sinths'}]</td>\n",
              "      <td></td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1197612190867476480</td>\n",
              "      <td>1197611882955325440</td>\n",
              "      <td>2019-11-21 20:26:11</td>\n",
              "      <td>2019-11-21 20:26:11</td>\n",
              "      <td>UTC</td>\n",
              "      <td></td>\n",
              "      <td>He looks allot like you. Old and white.\\n\\n#OkBoomer</td>\n",
              "      <td>[#okboomer]</td>\n",
              "      <td>[]</td>\n",
              "      <td>1005516908181925888</td>\n",
              "      <td>1005516908181925888</td>\n",
              "      <td>DustFar</td>\n",
              "      <td>FarThrustStarDust</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>https://twitter.com/DustFar/status/1197612190867476481</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>#okboomer</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[{'user_id': '1005516908181925888', 'username': 'DustFar'}, {'user_id': '113713991', 'username': 'timstao'}]</td>\n",
              "      <td></td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1197611782669402112</td>\n",
              "      <td>1197611782669402112</td>\n",
              "      <td>2019-11-21 20:24:34</td>\n",
              "      <td>2019-11-21 20:24:34</td>\n",
              "      <td>UTC</td>\n",
              "      <td></td>\n",
              "      <td>wait is my university’s president gaslighting me? #OkBoomer  pic.twitter.com/M2FNkQvKHo</td>\n",
              "      <td>[#okboomer]</td>\n",
              "      <td>[]</td>\n",
              "      <td>2509106274</td>\n",
              "      <td>2509106274</td>\n",
              "      <td>summerash99</td>\n",
              "      <td>queer, sultry summer</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>https://twitter.com/summerash99/status/1197611782669402113</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>#okboomer</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[{'user_id': '2509106274', 'username': 'summerash99'}]</td>\n",
              "      <td></td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1197611614687637504</td>\n",
              "      <td>1197611614687637504</td>\n",
              "      <td>2019-11-21 20:23:54</td>\n",
              "      <td>2019-11-21 20:23:54</td>\n",
              "      <td>UTC</td>\n",
              "      <td></td>\n",
              "      <td>Shut up Conway you whiny, fragile fossil. #okboomer #expirealready</td>\n",
              "      <td>[#okboomer, #expirealready]</td>\n",
              "      <td>[]</td>\n",
              "      <td>2789202068</td>\n",
              "      <td>2789202068</td>\n",
              "      <td>Crayondroids</td>\n",
              "      <td>Crayondroids</td>\n",
              "      <td>6</td>\n",
              "      <td>22</td>\n",
              "      <td>https://twitter.com/Crayondroids/status/1197611614687637504</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>#okboomer</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[{'user_id': '2789202068', 'username': 'Crayondroids'}]</td>\n",
              "      <td></td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    id      conversation_id  ... retweet_date lang\n",
              "0  1197612747715837952  1197612747715837952  ...               en \n",
              "1  1197612550403297280  1197612550403297280  ...               en \n",
              "2  1197612190867476480  1197611882955325440  ...               en \n",
              "3  1197611782669402112  1197611782669402112  ...               en \n",
              "4  1197611614687637504  1197611614687637504  ...               en \n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D380TCKbQEmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reset index (not really needed but why not)\n",
        "boomer_tweets_df = boomer_tweets_df.set_index(pd.to_datetime(boomer_tweets_df.created_at))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE9vzQN_QY5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's identify people Trump likes to mention\n",
        "boomer_tweets_df['mentions'] = boomer_tweets_df['tweet'].map(lambda textline: [tag for tag in tknzr.tokenize(textline) if tag.startswith('#')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj7MNCwWRM35",
        "colab_type": "code",
        "outputId": "c1983fae-8d94-44da-bb69-b22d7b1c1e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "boomer_tweets_df['mentions']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "created_at\n",
              "2019-11-21 20:28:24    [#OkBoomer]                \n",
              "2019-11-21 20:27:37    [#OkBoomer, #GenX]         \n",
              "2019-11-21 20:26:11    [#OkBoomer]                \n",
              "2019-11-21 20:24:34    [#OkBoomer]                \n",
              "2019-11-21 20:23:54    [#okboomer, #expirealready]\n",
              "                                  ...             \n",
              "2019-11-11 13:12:53    [#OkBoomer]                \n",
              "2019-11-11 13:11:46    [#OkBoomer]                \n",
              "2019-11-11 13:09:38    [#OkBoomer]                \n",
              "2019-11-11 13:09:11    [#OkBoomer, #OkCapitalist] \n",
              "2019-11-11 13:06:11    [#OKBoomer]                \n",
              "Name: mentions, Length: 9722, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9vnCHI0RlD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only keep tweets where a mention i present\n",
        "boomer_tweets_df = boomer_tweets_df[boomer_tweets_df['mentions'].map(len) > 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYL4GJBFRxy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Collect\n",
        "boomer_tags = itertools.chain(*boomer_tweets_df['mentions'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK509HPkR3SW",
        "colab_type": "code",
        "outputId": "c09c8ee9-e992-4dbb-8aec-36bc69cbfc88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "# Count up and show\n",
        "counted_tags = Counter(boomer_tags)\n",
        "counted_tags.most_common()[:20]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('#OkBoomer', 6254),\n",
              " ('#okboomer', 2362),\n",
              " ('#OKBoomer', 799),\n",
              " ('#Millennials', 181),\n",
              " ('#Boomer', 164),\n",
              " ('#okboomers', 148),\n",
              " ('#GenX', 143),\n",
              " ('#OKboomer', 132),\n",
              " ('#oktrumper', 126),\n",
              " ('#boomer', 116),\n",
              " ('#OkMillennial', 108),\n",
              " ('#GenZ', 106),\n",
              " ('#boomers', 91),\n",
              " ('#OKBOOMER', 80),\n",
              " ('#okBoomer', 77),\n",
              " ('#Boom25', 76),\n",
              " ('#millennials', 66),\n",
              " ('#millenials', 63),\n",
              " ('#okayboomer', 56),\n",
              " ('#DemDebate', 54)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk1PL2bWBixT",
        "colab_type": "text"
      },
      "source": [
        "Perform an LDA analysis of the #OKBoomer dataset\n",
        "\n",
        "Filter the corpus using tweet-preprocessor - try to figure out how to use it using it's documentation\n",
        "Clean up further with SpaCy (keep only ADV, ADJ, NOUN)\n",
        "Use Gensim to build a Dictionary (Filter extremes) and Corpus\n",
        "Use Gensim to run LDA\n",
        "Identify 10 topics\n",
        "Plot topic-counts by day"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqrs-t-5JM3V",
        "colab_type": "code",
        "outputId": "cbf46e0c-d642-46f8-abc4-4b1f6f28801b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "!pip install tweet-preprocessor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/2a/f8/810ec35c31cca89bc4f1a02c14b042b9ec6c19dd21f7ef1876874ef069a6/tweet-preprocessor-0.5.0.tar.gz\n",
            "Building wheels for collected packages: tweet-preprocessor\n",
            "  Building wheel for tweet-preprocessor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tweet-preprocessor: filename=tweet_preprocessor-0.5.0-cp36-none-any.whl size=7947 sha256=613b22fec134056ca5640f3dfb85b5823ef8da3596aa5adda57b013a4bfe6d7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/27/cc/49938e98a2470802ebdefae9d2b3f524768e970c1ebbe2dc4a\n",
            "Successfully built tweet-preprocessor\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTpTGQ25K2km",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import preprocessor as p\n",
        "p.set_options(p.OPT.URL,p.OPT.EMOJI)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY4Me3FJLuAy",
        "colab_type": "code",
        "outputId": "4d5abd9b-7066-4443-dbee-a9e858145287",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "# download and open some #OKBoomer tweets from trump_tweet_data_archive\n",
        "\n",
        "boomer_tweets_df = pd.read_json('https://github.com/CALDISS-AAU/sdsphd19_coursematerials/raw/master/data/tweets_boomer.zip')\n",
        "boomer_tweets_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>date</th>\n",
              "      <th>timezone</th>\n",
              "      <th>place</th>\n",
              "      <th>tweet</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>cashtags</th>\n",
              "      <th>user_id</th>\n",
              "      <th>user_id_str</th>\n",
              "      <th>username</th>\n",
              "      <th>name</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>link</th>\n",
              "      <th>retweet</th>\n",
              "      <th>nlikes</th>\n",
              "      <th>nreplies</th>\n",
              "      <th>nretweets</th>\n",
              "      <th>quote_url</th>\n",
              "      <th>search</th>\n",
              "      <th>near</th>\n",
              "      <th>geo</th>\n",
              "      <th>source</th>\n",
              "      <th>user_rt_id</th>\n",
              "      <th>user_rt</th>\n",
              "      <th>retweet_id</th>\n",
              "      <th>reply_to</th>\n",
              "      <th>retweet_date</th>\n",
              "      <th>lang</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1197612747715837952</td>\n",
              "      <td>1197612747715837952</td>\n",
              "      <td>2019-11-21 20:28:24</td>\n",
              "      <td>2019-11-21 20:28:24</td>\n",
              "      <td>UTC</td>\n",
              "      <td></td>\n",
              "      <td>Suddenly, #OkBoomer is trending again.  https:...</td>\n",
              "      <td>[#okboomer]</td>\n",
              "      <td>[]</td>\n",
              "      <td>262050686</td>\n",
              "      <td>262050686</td>\n",
              "      <td>DanClarkSports</td>\n",
              "      <td>Dan</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>https://twitter.com/DanClarkSports/status/1197...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>https://twitter.com/Fox35Matt/status/119718535...</td>\n",
              "      <td>#okboomer</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[{'user_id': '262050686', 'username': 'DanClar...</td>\n",
              "      <td></td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1197612550403297280</td>\n",
              "      <td>1197612550403297280</td>\n",
              "      <td>2019-11-21 20:27:37</td>\n",
              "      <td>2019-11-21 20:27:37</td>\n",
              "      <td>UTC</td>\n",
              "      <td></td>\n",
              "      <td>I like my role in this 🤷‍♂️ #OkBoomer #GenX pi...</td>\n",
              "      <td>[#okboomer, #genx]</td>\n",
              "      <td>[]</td>\n",
              "      <td>41444665</td>\n",
              "      <td>41444665</td>\n",
              "      <td>sinths</td>\n",
              "      <td>Sven Thomas</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>https://twitter.com/sinths/status/119761255040...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>#okboomer</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[{'user_id': '41444665', 'username': 'sinths'}]</td>\n",
              "      <td></td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1197612190867476480</td>\n",
              "      <td>1197611882955325440</td>\n",
              "      <td>2019-11-21 20:26:11</td>\n",
              "      <td>2019-11-21 20:26:11</td>\n",
              "      <td>UTC</td>\n",
              "      <td></td>\n",
              "      <td>He looks allot like you. Old and white.\\n\\n#Ok...</td>\n",
              "      <td>[#okboomer]</td>\n",
              "      <td>[]</td>\n",
              "      <td>1005516908181925888</td>\n",
              "      <td>1005516908181925888</td>\n",
              "      <td>DustFar</td>\n",
              "      <td>FarThrustStarDust</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>https://twitter.com/DustFar/status/11976121908...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>#okboomer</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[{'user_id': '1005516908181925888', 'username'...</td>\n",
              "      <td></td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1197611782669402112</td>\n",
              "      <td>1197611782669402112</td>\n",
              "      <td>2019-11-21 20:24:34</td>\n",
              "      <td>2019-11-21 20:24:34</td>\n",
              "      <td>UTC</td>\n",
              "      <td></td>\n",
              "      <td>wait is my university’s president gaslighting ...</td>\n",
              "      <td>[#okboomer]</td>\n",
              "      <td>[]</td>\n",
              "      <td>2509106274</td>\n",
              "      <td>2509106274</td>\n",
              "      <td>summerash99</td>\n",
              "      <td>queer, sultry summer</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>https://twitter.com/summerash99/status/1197611...</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>#okboomer</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[{'user_id': '2509106274', 'username': 'summer...</td>\n",
              "      <td></td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1197611614687637504</td>\n",
              "      <td>1197611614687637504</td>\n",
              "      <td>2019-11-21 20:23:54</td>\n",
              "      <td>2019-11-21 20:23:54</td>\n",
              "      <td>UTC</td>\n",
              "      <td></td>\n",
              "      <td>Shut up Conway you whiny, fragile fossil. #okb...</td>\n",
              "      <td>[#okboomer, #expirealready]</td>\n",
              "      <td>[]</td>\n",
              "      <td>2789202068</td>\n",
              "      <td>2789202068</td>\n",
              "      <td>Crayondroids</td>\n",
              "      <td>Crayondroids</td>\n",
              "      <td>6</td>\n",
              "      <td>22</td>\n",
              "      <td>https://twitter.com/Crayondroids/status/119761...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>#okboomer</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[{'user_id': '2789202068', 'username': 'Crayon...</td>\n",
              "      <td></td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    id      conversation_id  ... retweet_date lang\n",
              "0  1197612747715837952  1197612747715837952  ...                en\n",
              "1  1197612550403297280  1197612550403297280  ...                en\n",
              "2  1197612190867476480  1197611882955325440  ...                en\n",
              "3  1197611782669402112  1197611782669402112  ...                en\n",
              "4  1197611614687637504  1197611614687637504  ...                en\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PXNlftzQAtV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reset index (not really needed but why not)\n",
        "boomer_tweets_df = boomer_tweets_df.set_index(pd.to_datetime(boomer_tweets_df.created_at))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m03QLudXOyQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boomer_clean=boomer_tweets_df.tweet.map(p.clean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU1Tmv00PEek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr=TweetTokenizer(strip_handles=True)\n",
        "boomer_tweets_df['tokenized']=boomer_clean.map(lambda t:tknzr.tokenize(t))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZfiYy0-PaZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boomer_tweets_df['tokenized']=boomer_tweets_df['tokenized'].map(lambda t:[token.strip('@') for token in t if token.isalpha()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVCcwa_cOON7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34yj7t3_PkjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now, let's combine everything that we learned about preprocessing in a few lines of code\n",
        "\n",
        "tokens = []\n",
        "\n",
        "for summary in nlp.pipe(boomer_tweets_df['tweet']):\n",
        "  proj_tok = [token.lemma_.lower() for token in summary if token.pos_ in ['NOUN', 'ADJ', 'ADV'] and not token.is_stop] \n",
        "  tokens.append(proj_tok)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIErvDrGPmzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boomer_tweets_df['tokenized_sp']=tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MfOnOmjQAXP",
        "colab_type": "code",
        "outputId": "8c4aa675-212a-4e90-db3e-e2a0dcd1281c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "boomer_tweets_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>date</th>\n",
              "      <th>timezone</th>\n",
              "      <th>place</th>\n",
              "      <th>tweet</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>cashtags</th>\n",
              "      <th>user_id</th>\n",
              "      <th>user_id_str</th>\n",
              "      <th>username</th>\n",
              "      <th>name</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>link</th>\n",
              "      <th>retweet</th>\n",
              "      <th>nlikes</th>\n",
              "      <th>nreplies</th>\n",
              "      <th>nretweets</th>\n",
              "      <th>quote_url</th>\n",
              "      <th>search</th>\n",
              "      <th>near</th>\n",
              "      <th>geo</th>\n",
              "      <th>source</th>\n",
              "      <th>user_rt_id</th>\n",
              "      <th>user_rt</th>\n",
              "      <th>retweet_id</th>\n",
              "      <th>reply_to</th>\n",
              "      <th>retweet_date</th>\n",
              "      <th>lang</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>tokenized_sp</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>created_at</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2019-11-21 20:28:24</th>\n",
              "      <td>1197612747715837952</td>\n",
              "      <td>1197612747715837952</td>\n",
              "      <td>2019-11-21 20:28:24</td>\n",
              "      <td>2019-11-21 20:28:24</td>\n",
              "      <td>UTC</td>\n",
              "      <td></td>\n",
              "      <td>Suddenly, #OkBoomer is trending again.  https:...</td>\n",
              "      <td>[#okboomer]</td>\n",
              "      <td>[]</td>\n",
              "      <td>262050686</td>\n",
              "      <td>262050686</td>\n",
              "      <td>DanClarkSports</td>\n",
              "      <td>Dan</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>https://twitter.com/DanClarkSports/status/1197...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>https://twitter.com/Fox35Matt/status/119718535...</td>\n",
              "      <td>#okboomer</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[{'user_id': '262050686', 'username': 'DanClar...</td>\n",
              "      <td></td>\n",
              "      <td>en</td>\n",
              "      <td>[Suddenly, is, trending, again]</td>\n",
              "      <td>[suddenly]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-11-21 20:27:37</th>\n",
              "      <td>1197612550403297280</td>\n",
              "      <td>1197612550403297280</td>\n",
              "      <td>2019-11-21 20:27:37</td>\n",
              "      <td>2019-11-21 20:27:37</td>\n",
              "      <td>UTC</td>\n",
              "      <td></td>\n",
              "      <td>I like my role in this 🤷‍♂️ #OkBoomer #GenX pi...</td>\n",
              "      <td>[#okboomer, #genx]</td>\n",
              "      <td>[]</td>\n",
              "      <td>41444665</td>\n",
              "      <td>41444665</td>\n",
              "      <td>sinths</td>\n",
              "      <td>Sven Thomas</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>https://twitter.com/sinths/status/119761255040...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>#okboomer</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[{'user_id': '41444665', 'username': 'sinths'}]</td>\n",
              "      <td></td>\n",
              "      <td>en</td>\n",
              "      <td>[I, like, my, role, in, this]</td>\n",
              "      <td>[role, #]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-11-21 20:26:11</th>\n",
              "      <td>1197612190867476480</td>\n",
              "      <td>1197611882955325440</td>\n",
              "      <td>2019-11-21 20:26:11</td>\n",
              "      <td>2019-11-21 20:26:11</td>\n",
              "      <td>UTC</td>\n",
              "      <td></td>\n",
              "      <td>He looks allot like you. Old and white.\\n\\n#Ok...</td>\n",
              "      <td>[#okboomer]</td>\n",
              "      <td>[]</td>\n",
              "      <td>1005516908181925888</td>\n",
              "      <td>1005516908181925888</td>\n",
              "      <td>DustFar</td>\n",
              "      <td>FarThrustStarDust</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>https://twitter.com/DustFar/status/11976121908...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>#okboomer</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[{'user_id': '1005516908181925888', 'username'...</td>\n",
              "      <td></td>\n",
              "      <td>en</td>\n",
              "      <td>[He, looks, allot, like, you, Old, and, white]</td>\n",
              "      <td>[allot, old, white, #]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-11-21 20:24:34</th>\n",
              "      <td>1197611782669402112</td>\n",
              "      <td>1197611782669402112</td>\n",
              "      <td>2019-11-21 20:24:34</td>\n",
              "      <td>2019-11-21 20:24:34</td>\n",
              "      <td>UTC</td>\n",
              "      <td></td>\n",
              "      <td>wait is my university’s president gaslighting ...</td>\n",
              "      <td>[#okboomer]</td>\n",
              "      <td>[]</td>\n",
              "      <td>2509106274</td>\n",
              "      <td>2509106274</td>\n",
              "      <td>summerash99</td>\n",
              "      <td>queer, sultry summer</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>https://twitter.com/summerash99/status/1197611...</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>#okboomer</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[{'user_id': '2509106274', 'username': 'summer...</td>\n",
              "      <td></td>\n",
              "      <td>en</td>\n",
              "      <td>[wait, is, my, university, s, president, gasli...</td>\n",
              "      <td>[university, president]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-11-21 20:23:54</th>\n",
              "      <td>1197611614687637504</td>\n",
              "      <td>1197611614687637504</td>\n",
              "      <td>2019-11-21 20:23:54</td>\n",
              "      <td>2019-11-21 20:23:54</td>\n",
              "      <td>UTC</td>\n",
              "      <td></td>\n",
              "      <td>Shut up Conway you whiny, fragile fossil. #okb...</td>\n",
              "      <td>[#okboomer, #expirealready]</td>\n",
              "      <td>[]</td>\n",
              "      <td>2789202068</td>\n",
              "      <td>2789202068</td>\n",
              "      <td>Crayondroids</td>\n",
              "      <td>Crayondroids</td>\n",
              "      <td>6</td>\n",
              "      <td>22</td>\n",
              "      <td>https://twitter.com/Crayondroids/status/119761...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>#okboomer</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[{'user_id': '2789202068', 'username': 'Crayon...</td>\n",
              "      <td></td>\n",
              "      <td>en</td>\n",
              "      <td>[Shut, up, Conway, you, whiny, fragile, fossil]</td>\n",
              "      <td>[fragile, fossil, okboomer, expirealready]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      id  ...                                tokenized_sp\n",
              "created_at                                ...                                            \n",
              "2019-11-21 20:28:24  1197612747715837952  ...                                  [suddenly]\n",
              "2019-11-21 20:27:37  1197612550403297280  ...                                   [role, #]\n",
              "2019-11-21 20:26:11  1197612190867476480  ...                      [allot, old, white, #]\n",
              "2019-11-21 20:24:34  1197611782669402112  ...                     [university, president]\n",
              "2019-11-21 20:23:54  1197611614687637504  ...  [fragile, fossil, okboomer, expirealready]\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWkya9uDP7hq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A1YKL0sBiDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -qq -U gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Uz3dAQGW0Ya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the dictionary builder\n",
        "from gensim.corpora.dictionary import Dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE_sscJzPy89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(boomer_tweets_df['tokenized_sp'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj1zQeizTa4M",
        "colab_type": "code",
        "outputId": "16f0ff84-f67b-4eaa-8976-654caeeb469b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "boomer_tweets_df['tokenized_sp']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "created_at\n",
              "2019-11-21 20:28:24                                    [suddenly]\n",
              "2019-11-21 20:27:37                                     [role, #]\n",
              "2019-11-21 20:26:11                        [allot, old, white, #]\n",
              "2019-11-21 20:24:34                       [university, president]\n",
              "2019-11-21 20:23:54    [fragile, fossil, okboomer, expirealready]\n",
              "                                          ...                    \n",
              "2019-11-11 13:12:53                                            []\n",
              "2019-11-11 13:11:46                                [good, thread]\n",
              "2019-11-11 13:09:38           [boomer, not, new, boss, old, boss]\n",
              "2019-11-11 13:09:11                                     [room, #]\n",
              "2019-11-11 13:06:11                             [legit, okboomer]\n",
              "Name: tokenized_sp, Length: 9722, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9b3VB8ePzP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filter out low-frequency / high-frequency stuff, also limit the vocabulary to max 1000 words\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kWUJzxEPzUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# construct corpus using this dictionary\n",
        "corpus = [dictionary.doc2bow(doc) for doc in boomer_tweets_df['tokenized_sp']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj5is-uLPzNg",
        "colab_type": "code",
        "outputId": "e5cd388a-7d63-4365-b5f3-283e5a8c51f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# That's how the corpus looks\n",
        "corpus[10][:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 1), (5, 1), (18, 1), (19, 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN53weMxUj1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we'll use the faster multicore version of LDA\n",
        "\n",
        "from gensim.models import LdaMulticore"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW4OTHs-UlNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training the model (makes some mess atm due to version clashes)\n",
        "\n",
        "lda_model = LdaMulticore(corpus, id2word=dictionary,  num_topics=10, workers = 4, passes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoYAizZkUyf4",
        "colab_type": "code",
        "outputId": "f22bfbad-b841-4cde-f840-7101c42c3af8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "# Check out topics\n",
        "lda_model.print_topics(-1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.225*\"boomer\" + 0.126*\"okboomer\" + 0.068*\"ok\" + 0.063*\"#\" + 0.012*\"baby\" + 0.010*\"kid\" + 0.009*\"insult\" + 0.009*\"x\" + 0.008*\"%\" + 0.008*\"tweet\"'),\n",
              " (1,\n",
              "  '0.076*\"okboomer\" + 0.071*\"boomer\" + 0.033*\"man\" + 0.030*\"️\" + 0.024*\"white\" + 0.018*\"people\" + 0.016*\"snowflake\" + 0.013*\"lol\" + 0.012*\"old\" + 0.012*\"angry\"'),\n",
              " (2,\n",
              "  '0.074*\"generation\" + 0.050*\"time\" + 0.036*\"boomer\" + 0.036*\"okboomer\" + 0.035*\"right\" + 0.029*\"millenial\" + 0.022*\"thing\" + 0.018*\"fact\" + 0.018*\"#\" + 0.017*\"young\"'),\n",
              " (3,\n",
              "  '0.063*\"year\" + 0.043*\"okboomer\" + 0.034*\"#\" + 0.032*\"real\" + 0.024*\"job\" + 0.020*\"boomer\" + 0.016*\"phrase\" + 0.016*\"not\" + 0.013*\"time\" + 0.013*\"millennial\"'),\n",
              " (4,\n",
              "  '0.088*\"way\" + 0.068*\"okboomer\" + 0.068*\"#\" + 0.049*\"boomer\" + 0.043*\"new\" + 0.041*\"millennial\" + 0.018*\"purchase\" + 0.017*\"congratulations\" + 0.012*\"work\" + 0.011*\"not\"'),\n",
              " (5,\n",
              "  '0.113*\"not\" + 0.043*\"okboomer\" + 0.034*\"meme\" + 0.028*\"boomer\" + 0.023*\"🤣\" + 0.020*\"time\" + 0.015*\"actually\" + 0.015*\"people\" + 0.014*\"millennial\" + 0.013*\"thing\"'),\n",
              " (6,\n",
              "  '0.350*\"#\" + 0.102*\"okboomer\" + 0.053*\"boomer\" + 0.020*\"not\" + 0.013*\"millennial\" + 0.011*\"funny\" + 0.009*\"thing\" + 0.009*\"meme\" + 0.006*\"mom\" + 0.006*\"“\"'),\n",
              " (7,\n",
              "  '0.045*\"generation\" + 0.041*\"okboomer\" + 0.034*\"not\" + 0.025*\"young\" + 0.021*\"old\" + 0.020*\"people\" + 0.019*\"parent\" + 0.018*\"#\" + 0.017*\"problem\" + 0.015*\"pink\"'),\n",
              " (8,\n",
              "  '0.080*\"people\" + 0.059*\"okboomer\" + 0.057*\"old\" + 0.026*\"day\" + 0.023*\"good\" + 0.020*\"money\" + 0.013*\"kid\" + 0.013*\"fuck\" + 0.012*\"#\" + 0.012*\"instead\"'),\n",
              " (9,\n",
              "  '0.058*\"okboomer\" + 0.041*\"thing\" + 0.041*\"today\" + 0.037*\"age\" + 0.027*\"people\" + 0.023*\"old\" + 0.015*\"#\" + 0.014*\"boomer\" + 0.014*\"guy\" + 0.012*\"word\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw_lVVTAsZG0",
        "colab_type": "text"
      },
      "source": [
        "The site https://faketrump.ai/ is an interesting example of AI-powered fake-text generation. They write:\n",
        "\n",
        "We built an artificial intelligence model by fine-tuning GPT-2 to generate tweets in the style of Donald Trump’s Twitter account. After seeing the results, we also built a discriminator that can accurately detect fake tweets 77% of the time — think you can beat our classifier? Try it yourself!\n",
        "\n",
        "GPT-2 is a neural transformer-based model, that has been announced by OpenAI in February 2019 and created considerable discussion because they decided - in contrast to their earlier policies - not to release the mode to the public. Their central argument was that the model could be used to produce fake news, spam and alike too easily. The footnote of the faketrump page reads: “Generating realistic fake text has become much more accessible. We hope to highlight the current state of text generation to demonstrate how difficult it is to discern fiction from reality.”\n",
        "\n",
        "Since then several organizations and researchers have shown that it is possible to develop systems to detect “fake text”. We believe that you too can implement a competitive system.\n",
        "\n",
        "This assignment is not about Natural Language Processing (NLP) but about being able to deal with sequential data using deep learning. Some basic knowledge from M2 can be useful to squeeze the last 1% performance but you should be able to get great results with pure Keras. The data can be found here and has the following format:\n",
        "\n",
        "tweet labels string boolean There are 8000 real Trump tweet and 7348 fake ones.\n",
        "\n",
        "https://raw.githubusercontent.com/DeepLearnI/trump_tweet_classifier/master/code/tweet_labels.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdvIyPOCtf7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ3pjt-Qsa2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_df = pd.read_csv(\"https://raw.githubusercontent.com/DeepLearnI/trump_tweet_classifier/master/code/tweet_labels.csv\", sep=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S1IHLPwth1N",
        "colab_type": "code",
        "outputId": "19cc4c0f-350b-459f-dbe5-77484621c25f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "data_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>To every one of the HEROES we recognized today...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>We have been serving as policemen in Afghanist...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>...the Economy, where there is NO Recession, m...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>...the look of turmoil in the White House, of ...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A lot of Fake News is being reported that I ov...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  labels\n",
              "0  To every one of the HEROES we recognized today...    True\n",
              "1  We have been serving as policemen in Afghanist...    True\n",
              "2  ...the Economy, where there is NO Recession, m...    True\n",
              "3  ...the look of turmoil in the White House, of ...    True\n",
              "4  A lot of Fake News is being reported that I ov...    True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbVOMSLyKYsB",
        "colab_type": "code",
        "outputId": "682b5eb6-9062-46e6-8cde-a4194b1c0771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CanUEIMeKeQm",
        "colab_type": "code",
        "outputId": "3ecae703-5358-4a06-b04f-a7f19a81469f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip install -qq -U gensim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 24.2MB 92kB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erRrpZ4LLZb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJhRL_6AKkiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the dictionary builder\n",
        "from gensim.corpora.dictionary import Dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIoZVT5PKlIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the TfidfModel from Gensim\n",
        "from gensim.models.tfidfmodel import TfidfModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nfm97vg0Kpgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Just like before, we import the model\n",
        "from gensim.models.lsimodel import LsiModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPNY9LKtKwrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import stopwords\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OowfqqSpLhkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data using the train_test_split module. We keep 20% of the data for testing and use 80% to train the model\n",
        "# Random state defined with an arbitrary number for reproducibility\n",
        "train_df, test_df = train_test_split(data_df, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z6qYoH1KzYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize our texts and remove stopwords, also kick out numbers, lower everything\n",
        "\n",
        "train_tokens = train_df['tweet'].map(lambda t: [tok.lower() for tok in word_tokenize(t) if tok not in stop_words and tok.isalpha()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHkqRB9PK0Kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_tokens = test_df['tweet'].map(lambda t: [tok.lower() for tok in word_tokenize(t) if tok not in stop_words and tok.isalpha()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fMtbRdTK5p2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate a dictionary\n",
        "dictionary = Dictionary(train_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mlRofG6K7tv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filter it for extreme stuff\n",
        "dictionary.filter_extremes(no_below = 10, no_above=0.4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evrHUYfvK_vN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# construct corpus using this dictionary\n",
        "train_corpus = [dictionary.doc2bow(doc) for doc in train_tokens]\n",
        "test_corpus = [dictionary.doc2bow(doc) for doc in test_tokens]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D17aB6WDLB7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tooling to map between corpus (gensim) and matrix - more general\n",
        "from gensim.matutils import corpus2csc, corpus2dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgmx5SkuLESD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = corpus2csc(train_corpus)\n",
        "X_test = corpus2csc(test_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHGC6cHPLJLD",
        "colab_type": "code",
        "outputId": "7bd84304-eafd-4b18-f50d-3efbd918c06c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "model.fit(X_train.T, train_df.labels)\n",
        "\n",
        "model.score(X_test.T, test_df.labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7055374592833876"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}